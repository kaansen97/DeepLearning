{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JILVek0PhzbT"
      },
      "source": [
        "# Helper Code for Assignment 3 (RNN language models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGxdOkx_htjZ"
      },
      "source": [
        "## Reading raw text file & Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EawFomcOh3XE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import numpy.ma as ma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5A5DmY30hHHP"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "\n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "\n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1\n",
        "\n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id\n",
        "\n",
        "\n",
        "# Read the raw txt file and generate a 1D PyTorch tensor\n",
        "# containing the whole text mapped to sequence of token IDs, and a vocab object.\n",
        "class TextData:\n",
        "\n",
        "    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):\n",
        "        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)\n",
        "\n",
        "    def __len__(self):\n",
        "        print(len(self.data))\n",
        "        return len(self.data)\n",
        "\n",
        "    def text_to_data(self, text_file, vocab, extend_vocab, device):\n",
        "        \"\"\"Read a raw text file and create its tensor and the vocab.\n",
        "\n",
        "        Args:\n",
        "          text_file: a path to a raw text file.\n",
        "          vocab: a Vocab object\n",
        "          extend_vocab: bool, if True extend the vocab\n",
        "          device: device\n",
        "\n",
        "        Returns:\n",
        "          Tensor representing the input text, vocab file\n",
        "\n",
        "        \"\"\"\n",
        "        assert os.path.exists(text_file)\n",
        "        if vocab is None:\n",
        "            vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "        num_lines=0\n",
        "        number_chars=0\n",
        "        capital=0\n",
        "        lowcase=0\n",
        "\n",
        "\n",
        "        # Construct data\n",
        "        full_text = []\n",
        "        print(f\"Reading text file from: {text_file}\")\n",
        "        with open(text_file, 'r') as text:\n",
        "            for line in text:\n",
        "                tokens = list(line)\n",
        "                num_lines+=1\n",
        "                for token in tokens:\n",
        "                    number_chars+=1\n",
        "                    # get index will extend the vocab if the input\n",
        "                    # token is not yet part of the text.\n",
        "                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "\n",
        "                    if token.isupper():\n",
        "                      capital+=1\n",
        "                    if token.islower():\n",
        "                      lowcase+=1\n",
        "        # convert to tensor\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "        print(\"Number of lines {}\".format(num_lines))\n",
        "        print(\"Number of character {}\".format(number_chars))\n",
        "        print(\"Number of Capital Letters number {}\".format(capital))\n",
        "        print(\"Number of Lower Letters number {}\".format(lowcase))\n",
        "        print(\"Number of Unique Character {}\".format(len(vocab.string_to_id)))\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data, vocab\n",
        "\n",
        "\n",
        "# Since there is no need for schuffling the data, we just have to split\n",
        "# the text data according to the batch size and bptt length.\n",
        "# The input to be fed to the model will be batch[:-1]\n",
        "# The target to be used for the loss will be batch[1:]\n",
        "class DataBatches:\n",
        "\n",
        "    def __init__(self, data, bsz, bptt_len, pad_id):\n",
        "        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "    def create_batch(self, input_data, bsz, bptt_len, pad_id):\n",
        "        \"\"\"Create batches from a TextData object .\n",
        "\n",
        "        Args:\n",
        "          input_data: a TextData object.\n",
        "          bsz: int, batch size\n",
        "          bptt_len: int, bptt length\n",
        "          pad_id: int, ID of the padding token\n",
        "\n",
        "        Returns:\n",
        "          List of tensors representing batches\n",
        "\n",
        "        \"\"\"\n",
        "        batches = []  # each element in `batches` is (len, B) tensor\n",
        "        text_len = len(input_data)\n",
        "        segment_len = text_len // bsz + 1\n",
        "\n",
        "        # Question: Explain the next two lines!\n",
        "        padded = input_data.data.new_full((segment_len * bsz,), pad_id)\n",
        "        padded[:text_len] = input_data.data\n",
        "        padded = padded.view(bsz, segment_len).t()\n",
        "        num_batches = segment_len // bptt_len + 1\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            # Prepare batches such that the last symbol of the current batch\n",
        "            # is the first symbol of the next batch.\n",
        "            if i == 0:\n",
        "                # Append a dummy start symbol using pad token\n",
        "                batch = torch.cat(\n",
        "                    [padded.new_full((1, bsz), pad_id),\n",
        "                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)\n",
        "                batches.append(batch)\n",
        "            else:\n",
        "                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])\n",
        "\n",
        "        return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u-9Q7K4hVbL",
        "outputId": "7914f735-af0c-4cc4-a159-a5454f878d05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-04 12:22:49--  http://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://www.gutenberg.org/files/49010/49010-0.txt [following]\n",
            "--2022-12-04 12:22:50--  https://www.gutenberg.org/files/49010/49010-0.txt\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 185303 (181K) [text/plain]\n",
            "Saving to: ‘49010-0.txt.1’\n",
            "\n",
            "49010-0.txt.1       100%[===================>] 180.96K   596KB/s    in 0.3s    \n",
            "\n",
            "2022-12-04 12:22:50 (596 KB/s) - ‘49010-0.txt.1’ saved [185303/185303]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# downlaod the text\n",
        "# Make sure to go to the link and check how the text looks like.\n",
        "!wget http://www.gutenberg.org/files/49010/49010-0.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRqPHT_zhgNa"
      },
      "outputs": [],
      "source": [
        "# This is for Colab. Adapt the path if needed.\n",
        "text_path = \"/content/49010-0.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hff1FxeohHHR",
        "outputId": "1b5bb277-7467-420e-c808-088bf65ebf2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading text file from: /content/49010-0.txt\n",
            "Number of lines 5033\n",
            "Number of character 177517\n",
            "Number of Capital Letters number 9666\n",
            "Number of Lower Letters number 122224\n",
            "Number of Unique Character 107\n",
            "Done.\n",
            "177517\n"
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = TextData(text_path, device=DEVICE)\n",
        "batches = DataBatches(my_data, batch_size, bptt_len, pad_id=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3WITXfTc67"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxszGttKhHHU"
      },
      "outputs": [],
      "source": [
        "# RNN based language model\n",
        "class RNNModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hidden_dim, num_layers):\n",
        "        \"\"\"Parameters:\n",
        "\n",
        "          num_classes (int): number of input/output classes\n",
        "          emb_dim (int): token embedding size\n",
        "          hidden_dim (int): hidden layer size of RNNs\n",
        "          num_layers (int): number of RNN layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.input_layer = nn.Embedding(num_classes, emb_dim)\n",
        "        self.rnn = nn.RNN(emb_dim, hidden_dim, num_layers)\n",
        "        self.out_layer = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, input, state):\n",
        "        emb = self.input_layer(input)\n",
        "        output, state = self.rnn(emb, state)\n",
        "        output = self.out_layer(output)\n",
        "        output = output.view(-1, self.num_classes)\n",
        "        return output, state\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)\n",
        "\n",
        "\n",
        "# To be modified for LSTM...\n",
        "def custom_detach(h):\n",
        "    return h.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyW5_JUXTgAV"
      },
      "source": [
        "## Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aA5DRnBAhHHV"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def complete(model, prompt, steps, sample=False):\n",
        "    \"\"\"Complete the prompt for as long as given steps using the model.\n",
        "\n",
        "    Parameters:\n",
        "      model: language model\n",
        "      prompt (str): text segment to be completed\n",
        "      steps (int): number of decoding steps.\n",
        "      sample (bool): If True, sample from the model. Otherwise greedy.\n",
        "\n",
        "    Returns:\n",
        "      completed text (str)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    out_list = []\n",
        "\n",
        "    # forward the prompt, compute prompt's ppl\n",
        "    prompt_list = []\n",
        "    char_prompt = list(prompt)\n",
        "    for char in char_prompt:\n",
        "        prompt_list.append(my_data.vocab.string_to_id[char])\n",
        "    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)\n",
        "\n",
        "    states = model.init_hidden(1)\n",
        "    logits, states = model(x, states)\n",
        "    probs = F.softmax(logits[-1], dim=-1)\n",
        "\n",
        "    if sample:\n",
        "        ix=torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    else:\n",
        "        max_p, ix = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "    out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "    x = ix.unsqueeze(1)\n",
        "\n",
        "    # decode\n",
        "    for k in range(steps):\n",
        "        logits, states = model(x, states)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        if sample:  # sample from the distribution or take the most likely\n",
        "             ix=torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix\n",
        "    return ''.join(out_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaVkFEz8hHHV",
        "outputId": "99a60c49-18e5-4ab2-a0fc-041f95d4b3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 107\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.0005\n",
        "clipping = 1.0\n",
        "embedding_size = 64\n",
        "rnn_size = 2048\n",
        "rnn_num_layers = 1\n",
        "\n",
        "# vocab_size = len(module.vocab.itos)\n",
        "vocab_size = len(my_data.vocab.id_to_string)\n",
        "print(F\"vocab size: {vocab_size}\")\n",
        "\n",
        "model = RNNModel(\n",
        "    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=rnn_size,\n",
        "    num_layers=rnn_num_layers)\n",
        "model = model.to(DEVICE)\n",
        "hidden = model.init_hidden(batch_size)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bhimbs4T0dn"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xTOfBUIhHHW",
        "outputId": "02e46f54-4375-4ce3-c879-9d4d562d554f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== start epoch 0 ===\n",
            "train ppl: 107.58854675292969\n",
            "----------------- epoch/batch 0/0 -----------------\n",
            "Dogs like best to\n",
            "                                                                                                                                 \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 13.570418357849121\n",
            "----------------- epoch/batch 0/30 -----------------\n",
            "Dogs like best to\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 11.549098014831543\n",
            "----------------- epoch/batch 0/60 -----------------\n",
            "Dogs like best to\n",
            " the se the sed the sored the sored the sored the sored the sored the sored the sored the sored the sored the sored the sored the\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 1 ===\n",
            "train ppl: 10.185006141662598\n",
            "----------------- epoch/batch 1/0 -----------------\n",
            "Dogs like best to\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 9.01037311553955\n",
            "----------------- epoch/batch 1/30 -----------------\n",
            "Dogs like best to\n",
            " the soon the soon the soon the soon the soon the soon the soon the soon the soon the soon the soon the soon the soon the soon th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 8.988958358764648\n",
            "----------------- epoch/batch 1/60 -----------------\n",
            "Dogs like best to\n",
            " the wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat he wat h\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 2 ===\n",
            "train ppl: 8.082773208618164\n",
            "----------------- epoch/batch 2/0 -----------------\n",
            "Dogs like best to\n",
            " the wall a done the was a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 7.426371097564697\n",
            "----------------- epoch/batch 2/30 -----------------\n",
            "Dogs like best to\n",
            " be and the Wolf the sand the Wolf the sand the Wolf the sand the Wolf the sand the Wolf the sand the Wolf the sand the Wolf the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 7.70973539352417\n",
            "----------------- epoch/batch 2/60 -----------------\n",
            "Dogs like best to\n",
            " the way the warked the way the warked the way the warked the way the warked the way the warked the way the warked the way the wa\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 3 ===\n",
            "train ppl: 6.953096866607666\n",
            "----------------- epoch/batch 3/0 -----------------\n",
            "Dogs like best to\n",
            " the ground a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the F\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 6.504138946533203\n",
            "----------------- epoch/batch 3/30 -----------------\n",
            "Dogs like best to\n",
            " and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wol\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 6.782264709472656\n",
            "----------------- epoch/batch 3/60 -----------------\n",
            "Dogs like best to\n",
            " the wat the wat the was the wat the was the wat the was the wat the was the wat the was the wat the was the wat the was the wat \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 4 ===\n",
            "train ppl: 6.121280670166016\n",
            "----------------- epoch/batch 4/0 -----------------\n",
            "Dogs like best to\n",
            " the man a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox a said the Fox \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.87044620513916\n",
            "----------------- epoch/batch 4/30 -----------------\n",
            "Dogs like best to\n",
            " me the same him and the Wolf, “I will the same had beting the same him and the Wolf, “I will the same had beting the same him an\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 6.106062412261963\n",
            "----------------- epoch/batch 4/60 -----------------\n",
            "Dogs like best to\n",
            " the water the water the water the water the water the water the water the water the water the water the water the water the wate\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 5 ===\n",
            "train ppl: 5.50530481338501\n",
            "----------------- epoch/batch 5/0 -----------------\n",
            "Dogs like best to\n",
            " the count a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fo\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.361079692840576\n",
            "----------------- epoch/batch 5/30 -----------------\n",
            "Dogs like best to\n",
            " me the server he had been his before the same the same the same the same the same the same the same the same the same the same t\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.554203987121582\n",
            "----------------- epoch/batch 5/60 -----------------\n",
            "Dogs like best to\n",
            " the water was a single the many and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and the Wolf and t\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 6 ===\n",
            "train ppl: 5.010952472686768\n",
            "----------------- epoch/batch 6/0 -----------------\n",
            "Dogs like best to\n",
            " the great a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fo\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.95081901550293\n",
            "----------------- epoch/batch 6/30 -----------------\n",
            "Dogs like best to\n",
            " me the was to the forest the same the was to the tome to the forms of the forest the same the was to the tome to the forms of th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.04429292678833\n",
            "----------------- epoch/batch 6/60 -----------------\n",
            "Dogs like best to\n",
            " the country would not bean his hand and the Country would not bean his hand and the Country would not bean his hand and the Coun\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 7 ===\n",
            "train ppl: 4.634677410125732\n",
            "----------------- epoch/batch 7/0 -----------------\n",
            "Dogs like best to\n",
            " the edest the made a there is not a Fox the man a Fox the man a Fox the man a Fox the man a Fox the man a Fox the man a Fox the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.6207098960876465\n",
            "----------------- epoch/batch 7/30 -----------------\n",
            "Dogs like best to\n",
            " be sounder the terms of the forest.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE DOG AND THE GRAN AND THE GRAN AND THE GRAN AND THE GRAN AND THE GRAN AND THE GRAN AN\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.7150774002075195\n",
            "----------------- epoch/batch 7/60 -----------------\n",
            "Dogs like best to\n",
            " the charce on the hard to his hand and the Cat last and the Cat last and the Cat last and the Cat last and the Cat last and the \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 8 ===\n",
            "train ppl: 4.344249725341797\n",
            "----------------- epoch/batch 8/0 -----------------\n",
            "Dogs like best to\n",
            " the great spowed to the great spowed to the great spowed to the great spowed to the great spowed to the great spowed to the grea\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.286949157714844\n",
            "----------------- epoch/batch 8/30 -----------------\n",
            "Dogs like best to\n",
            " make a little said the Gook and the Gook and the Gook and the Gook and the Gook and the Gook and the Gook and the Gook and the G\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.315384864807129\n",
            "----------------- epoch/batch 8/60 -----------------\n",
            "Dogs like best to\n",
            " the fortune the country where you have her when he could not better that he could not beting the she her good for the fortune th\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 9 ===\n",
            "train ppl: 4.0706329345703125\n",
            "----------------- epoch/batch 9/0 -----------------\n",
            "Dogs like best to\n",
            " the great spown a little started the Fox so was a Frog                                                                          \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.0734028816223145\n",
            "----------------- epoch/batch 9/30 -----------------\n",
            "Dogs like best to\n",
            " sat the same han the came and the Wolf and the Grasshoppers of the forest of a place of she was soon as the same han the came an\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.057483196258545\n",
            "----------------- epoch/batch 9/60 -----------------\n",
            "Dogs like best to\n",
            " the present them a King the man he was not a shall began to the person the beat his hand and the man he was not a shall began to\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 10 ===\n",
            "train ppl: 3.8274588584899902\n",
            "----------------- epoch/batch 10/0 -----------------\n",
            "Dogs like best to\n",
            " the ground, and a Frog                                                                                                          \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.8353140354156494\n",
            "----------------- epoch/batch 10/30 -----------------\n",
            "Dogs like best to\n",
            " his own and sting on the water said the Wolf, who was not and was donning and greating and greating and greating and greating an\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.8025765419006348\n",
            "----------------- epoch/batch 10/60 -----------------\n",
            "Dogs like best to\n",
            " eat it was grain to the water was so tain the man how hard to the water was so tain the man how hard to the water was so tain th\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 11 ===\n",
            "train ppl: 3.641674518585205\n",
            "----------------- epoch/batch 11/0 -----------------\n",
            "Dogs like best to\n",
            " the ground, and it is the world at the shorter to go that you must go donations or distribute the ground, and all the served to \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.5868542194366455\n",
            "----------------- epoch/batch 11/30 -----------------\n",
            "Dogs like best to\n",
            " his own and the Lion was donn for the forest the should go not come to the top of the forest to provide and propised to get out \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.5534329414367676\n",
            "----------------- epoch/batch 11/60 -----------------\n",
            "Dogs like best to\n",
            "\n",
            "strak him.\n",
            "\n",
            "“That is true,” said the Camel                                                                                      \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 12 ===\n",
            "train ppl: 3.370271682739258\n",
            "----------------- epoch/batch 12/0 -----------------\n",
            "Dogs like best to\n",
            " the ground, and it is to be before your for the befter than the more as a part of the pool, and the Fox                         \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.3888607025146484\n",
            "----------------- epoch/batch 12/30 -----------------\n",
            "Dogs like best to\n",
            " till the Morse and the Lion                                                                                                     \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.395839214324951\n",
            "----------------- epoch/batch 12/60 -----------------\n",
            "Dogs like best to\n",
            " she to be found in a shadow the tree, the tree, the tame                  166\n",
            "    The Wolf and the Cambl which the fable for the\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 13 ===\n",
            "train ppl: 3.1964645385742188\n",
            "----------------- epoch/batch 13/0 -----------------\n",
            "Dogs like best to\n",
            " his own place when he was allowed to the gold said he had lost at his own black.\n",
            "\n",
            "“Oh, now you are so large and the man be to he\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.2056124210357666\n",
            "----------------- epoch/batch 13/30 -----------------\n",
            "Dogs like best to\n",
            " the one on the world his companions, and ston be a dog, who was not a service their polushoppers companions, and ston be a dog, \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.2627742290496826\n",
            "----------------- epoch/batch 13/60 -----------------\n",
            "Dogs like best to\n",
            " she to the comparing that it was not a stream.\n",
            "\n",
            "Then he lotten into a formet in the country where you are a long to any beast as\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 14 ===\n",
            "train ppl: 3.0458757877349854\n",
            "----------------- epoch/batch 14/0 -----------------\n",
            "Dogs like best to\n",
            " his hand and do not so diglest the start from the publes and lived it happen, and the same man to the gods, and the Fox         \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.0796968936920166\n",
            "----------------- epoch/batch 14/30 -----------------\n",
            "Dogs like best to\n",
            " make him thers, and the Wolf a dight to be so give you the dry. The Wolf had come from a computers that you will sure you do not\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.179213047027588\n",
            "----------------- epoch/batch 14/60 -----------------\n",
            "Dogs like best to\n",
            " she was not a stream.\n",
            "\n",
            "Then he saw how straight forth the now what she said, “What growned the hound that were down upon the bor\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 15 ===\n",
            "train ppl: 2.79441499710083\n",
            "----------------- epoch/batch 15/0 -----------------\n",
            "Dogs like best to\n",
            " the edge of the most of his hand, and so was all down and see to get a shall be to the end of the most of his hand, and so was a\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.981088399887085\n",
            "----------------- epoch/batch 15/30 -----------------\n",
            "Dogs like best to\n",
            " make him thers it was the strong and grasshoppers which he had disposed to be some gunit of the work as long as strong and grans\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.9964475631713867\n",
            "----------------- epoch/batch 15/60 -----------------\n",
            "Dogs like best to\n",
            " such a tish so greater new so pracedent them and by say in the way you so grawey heave the works from the purties of the world h\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 16 ===\n",
            "train ppl: 2.641228199005127\n",
            "----------------- epoch/batch 16/0 -----------------\n",
            "Dogs like best to\n",
            " make the treed and the Horse as a commor of the dons, and the Hawk said to himself that came a great spend a holl and said to hi\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.8322224617004395\n",
            "----------------- epoch/batch 16/30 -----------------\n",
            "Dogs like best to\n",
            " make his thoused and day there who asked the Hawk to the thing with the terms of this agreement for keep.”\n",
            "\n",
            "“What a short them a\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.799806833267212\n",
            "----------------- epoch/batch 16/60 -----------------\n",
            "Dogs like best to\n",
            " steak as I have no noudertions against a Cambled his long neck, and begged the country where you are smalled the han being very \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 17 ===\n",
            "train ppl: 2.583127737045288\n",
            "----------------- epoch/batch 17/0 -----------------\n",
            "Dogs like best to\n",
            " eat your by an about a fain of the access to other came a full of see, and they all the terms of this agreement shall be is in s\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.654785633087158\n",
            "----------------- epoch/batch 17/30 -----------------\n",
            "Dogs like best to\n",
            " little pleasure and singer,” the man had been the dest his cale.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FOX AND THE LION\n",
            "\n",
            "\n",
            "A MOUSE AND THE LION\n",
            "\n",
            "\n",
            "A DOG once ha\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.7110230922698975\n",
            "----------------- epoch/batch 17/60 -----------------\n",
            "Dogs like best to\n",
            " eat it all the same man’s leg so gradefully stated with the one in the careful not to take a little fall into the water, and som\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 18 ===\n",
            "train ppl: 2.4877941608428955\n",
            "----------------- epoch/batch 18/0 -----------------\n",
            "Dogs like best to\n",
            " eat at once to come and help him our of the copy, and we hope weather a peal he should not me to the gree of the comparison of t\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.5655934810638428\n",
            "----------------- epoch/batch 18/30 -----------------\n",
            "Dogs like best to\n",
            " yin the tent or relieve you will servely some in the day to get him enough. What it was to\n",
            "      Stor on our fine were startled \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.5521910190582275\n",
            "----------------- epoch/batch 18/60 -----------------\n",
            "Dogs like best to\n",
            " eat it, they came in a full for its saw a Cucires and the Stork was she started at the sirns alond and redection to the ground, \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 19 ===\n",
            "train ppl: 2.3519551753997803\n",
            "----------------- epoch/batch 19/0 -----------------\n",
            "Dogs like best to\n",
            " his strength, the\n",
            "Project Gutenberg-tm electronic work, you must all the terms of this agreement. If a beggee should be for a wa\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.4777350425720215\n",
            "----------------- epoch/batch 19/30 -----------------\n",
            "Dogs like best to\n",
            " yill\n",
            "him to relich the preply, and the stork or any part of a return.”\n",
            "\n",
            "“You did not bit one this to a pity no and the other din\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.4617221355438232\n",
            "----------------- epoch/batch 19/60 -----------------\n",
            "Dogs like best to\n",
            " say that you are straight and injointed to anyoon along the highwing in the way home and lived indan along time, he said, “Ond h\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 20 ===\n",
            "train ppl: 2.2399275302886963\n",
            "----------------- epoch/batch 20/0 -----------------\n",
            "Dogs like best to\n",
            " make himself were was a leag herself out.\n",
            "\n",
            "“Som, while the Frogs, and the Swallow tried to\n",
            "frightened by the council tromined hi\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.3829190731048584\n",
            "----------------- epoch/batch 20/30 -----------------\n",
            "Dogs like best to\n",
            " make you supper, and to plare it was that the start, and the Wolf, the Wolf dang of replied that did not so nit lock, and could \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.333958864212036\n",
            "----------------- epoch/batch 20/60 -----------------\n",
            "Dogs like best to\n",
            " the precone to the precition of\n",
            "their companions all that had\n",
            "eaten heater of the works lasture, to the water was so good a good\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 21 ===\n",
            "train ppl: 2.1654632091522217\n",
            "----------------- epoch/batch 21/0 -----------------\n",
            "Dogs like best to\n",
            " make yourself able down came all mares. “I Can and her Hen Eat afreeder. The Frogs, and the Hare what a pitceed a dog, and said,\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.34548282623291\n",
            "----------------- epoch/batch 21/30 -----------------\n",
            "Dogs like best to\n",
            " mish the forest.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FOX AND THE LION\n",
            "\n",
            "\n",
            "A LITTLE for the pund, and for the land, and a think Fow, the Wolf before the acried\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.172023296356201\n",
            "----------------- epoch/batch 21/60 -----------------\n",
            "Dogs like best to\n",
            " make a good appearances are a look at it. They came alone for you.”\n",
            "\n",
            "The poor Man. “If you would lasted to a good plan. How good\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 22 ===\n",
            "train ppl: 2.1006033420562744\n",
            "----------------- epoch/batch 22/0 -----------------\n",
            "Dogs like best to\n",
            " make you said go he saw the Man and hard so there had no part in the water carried him way we shall he could not me put to death\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.2114739418029785\n",
            "----------------- epoch/batch 22/30 -----------------\n",
            "Dogs like best to\n",
            " me a dinner a the\n",
            "laws of the field. The\n",
            "pitce of his prey,\n",
            "and they are dilled out, but been\n",
            "the Came, the Wolf began to slink \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.1723716259002686\n",
            "----------------- epoch/batch 22/60 -----------------\n",
            "Dogs like best to\n",
            " make a good appearances are not locations and get to see in his masters once string in a tree to see if you let me go. It is the\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 23 ===\n",
            "train ppl: 1.9793986082077026\n",
            "----------------- epoch/batch 23/0 -----------------\n",
            "Dogs like best to\n",
            " make you are the agrans frightened the Cock slow ad a coples and herself out, saying, “All at once save her a pet them all and s\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.093623638153076\n",
            "----------------- epoch/batch 23/30 -----------------\n",
            "Dogs like best to\n",
            " me ach your own for their danger of his present of a pot with a strong and graceful the\n",
            "stage as he could get out again, he was \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.0685501098632812\n",
            "----------------- epoch/batch 23/60 -----------------\n",
            "Dogs like best to\n",
            " the greater in the world has need to his like this exem.\n",
            "\n",
            "“Why do into a time to the birno and seeing them easily. Then said to \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 24 ===\n",
            "train ppl: 1.9488892555236816\n",
            "----------------- epoch/batch 24/0 -----------------\n",
            "Dogs like best to\n",
            " eat it, and said, “the strong.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE MOUNE AND THE CEAN\n",
            "\n",
            "\n",
            "A FOX was once strecced\n",
            "for the drann, then, or here a befund from \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.077359676361084\n",
            "----------------- epoch/batch 24/30 -----------------\n",
            "Dogs like best to\n",
            " make up it was the attred as once caught in the birds were in ridation and discone, and now you in the dong the biddem the other\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.0046138763427734\n",
            "----------------- epoch/batch 24/60 -----------------\n",
            "Dogs like best to\n",
            " the tree, he said, “Ond\n",
            "has like to the Lion. “Yect to bleak there, and seeing them and back in the\n",
            "water.\n",
            "\n",
            "“Cat you can do but \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 25 ===\n",
            "train ppl: 1.8584673404693604\n",
            "----------------- epoch/batch 25/0 -----------------\n",
            "Dogs like best to\n",
            " the dogs. “Dow it is nothing that would not let any one eleched the searome, If hay conficted the land, and all at once the done\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.0204131603240967\n",
            "----------------- epoch/batch 25/30 -----------------\n",
            "Dogs like best to\n",
            " make a mouthful of\n",
            "him of his own upliet, and the Traveler                                                                      \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9224568605422974\n",
            "----------------- epoch/batch 25/60 -----------------\n",
            "Dogs like best to\n",
            " the field.\n",
            "\n",
            "After you look\n",
            "upon him.\n",
            "\n",
            "“Oh, not being dog.\n",
            "\n",
            "“The first to trying to see what was expected.\n",
            "\n",
            "The Fox caught upon\n",
            "a\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 26 ===\n",
            "train ppl: 1.767869234085083\n",
            "----------------- epoch/batch 26/0 -----------------\n",
            "Dogs like best to\n",
            " the others to disting him with one entertates coofordance. Mon, and seemed so of her hears, that it was any proposed to be any p\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9599356651306152\n",
            "----------------- epoch/batch 26/30 -----------------\n",
            "Dogs like best to\n",
            " make the legs to the folk, and the Fox                         115\n",
            "    The Wolf and the Sheep                                   \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9187008142471313\n",
            "----------------- epoch/batch 26/60 -----------------\n",
            "Dogs like best to\n",
            " the little one to his promisely.\n",
            "\n",
            "By as I have for difter. We looked far long legs wooless you would not think of servant. He wo\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 27 ===\n",
            "train ppl: 1.6811683177947998\n",
            "----------------- epoch/batch 27/0 -----------------\n",
            "Dogs like best to\n",
            " the other. The with any particular protice a different peach of confiesed the eagle stood up\n",
            "tall and sleep, state its long ears\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.8801724910736084\n",
            "----------------- epoch/batch 27/30 -----------------\n",
            "Dogs like best to\n",
            " make her you want a tous and made himself out the shuse and the Wolf            119\n",
            "    The Waw and the Swag, when he took his l\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9182634353637695\n",
            "----------------- epoch/batch 27/60 -----------------\n",
            "Dogs like best to\n",
            " the water.\n",
            "What would be to the leasts to the Wolf, af a granching of the Project\n",
            "Gutenberg-tm trademark of the Project\n",
            "Gutenber\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 28 ===\n",
            "train ppl: 1.6112123727798462\n",
            "----------------- epoch/batch 28/0 -----------------\n",
            "Dogs like best to\n",
            "\n",
            "distinguish that strong and proud.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE WOLF AND THE SHEEP\n",
            "\n",
            "\n",
            "A THE FOX AND THE GOAT\n",
            "\n",
            "\n",
            "AN Ant, which has sprended hard for hi\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.835540771484375\n",
            "----------------- epoch/batch 28/30 -----------------\n",
            "Dogs like best to\n",
            " make himself it, but his teath, the Lion was soth ule that that the Poos farrer,” said the Goat, “for you have removed and sate\n",
            "\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.8231381177902222\n",
            "----------------- epoch/batch 28/60 -----------------\n",
            "Dogs like best to\n",
            " the water, where I pal one by one to his took to his terrs what was expected, she said, “Ond had leap distribute as he was so go\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 29 ===\n",
            "train ppl: 1.5659781694412231\n",
            "----------------- epoch/batch 29/0 -----------------\n",
            "Dogs like best to\n",
            "\n",
            "difficulty into a provision of several streaming the roadside, roaming, but it\n",
            "eas, the work of Jysippus, one of the little snak\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.8220908641815186\n",
            "----------------- epoch/batch 29/30 -----------------\n",
            "Dogs like best to\n",
            " make him keating\n",
            "approof that there is the day and that he must be could will be\n",
            "by their holes and nothing that the Project Gut\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.724434733390808\n",
            "----------------- epoch/batch 29/60 -----------------\n",
            "Dogs like best to\n",
            " the fables may be built up from the sen and her negg and swallowed to have been by the side of a righ to eat it at lenster, and \n",
            "----------------- end generated text -------------------\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "num_epochs = 30\n",
        "report_every = 30\n",
        "prompt = \"Dogs like best to\"\n",
        "perp_list=[]\n",
        "gen_text=[]\n",
        "\n",
        "for ep in range(num_epochs):\n",
        "    print(f\"=== start epoch {ep} ===\")\n",
        "    state = model.init_hidden(batch_size)\n",
        "    for idx in range(len(batches)):\n",
        "        batch = batches[idx]\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        state = custom_detach(state)\n",
        "\n",
        "        input = batch[:-1]\n",
        "        target = batch[1:].reshape(-1)\n",
        "\n",
        "        bsz = input.shape[1]\n",
        "        prev_bsz = state.shape[1]\n",
        "        if bsz != prev_bsz:\n",
        "            state = state[:, :bsz, :]\n",
        "        output, state = model(input, state)\n",
        "        loss = loss_fn(output, target)\n",
        "        perp=torch.exp(loss)\n",
        "        perp_list.append(perp)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "        if idx % report_every == 0:\n",
        "            # print(f\"train loss: {loss.item()}\")  # replace me by the line below!\n",
        "            print(f\"train ppl: {perp}\")\n",
        "            if ((ep==5 or ep==15 or ep==28)and idx==0):\n",
        "                            generated_text = complete(model, prompt, 128, sample=False)\n",
        "                            gen_text.append(str(generated_text)+\"\\n\")\n",
        "\n",
        "            generated_text = complete(model, prompt, 128, sample=False)\n",
        "            print(f'----------------- epoch/batch {ep}/{idx} -----------------')\n",
        "            print(prompt)\n",
        "            print(generated_text)\n",
        "            print(f'----------------- end generated text -------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tS_1we7QN-R",
        "outputId": "5615aa85-38f5-49d8-ca0d-e9ec7c38a375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' the count a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fox a Fo\\n', ' the edge of the most of his hand, and so was all down and see to get a shall be to the end of the most of his hand, and so was a\\n', '\\ndistinguish that strong and proud.”\\n\\n\\n\\n\\nTHE WOLF AND THE SHEEP\\n\\n\\nA THE FOX AND THE GOAT\\n\\n\\nAN Ant, which has sprended hard for hi\\n']\n"
          ]
        }
      ],
      "source": [
        "print(gen_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKKZ63EaS-_C"
      },
      "outputs": [],
      "source": [
        "perp=[t.to(\"cpu\").detach().numpy() for t in perp_list]\n",
        "perp=np.stack(perp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "P9LwmkCRZFz5",
        "outputId": "848288df-6e58-41b2-8abf-833bf16b1c3e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"edc55e2c-231c-4186-8e75-b42496bfafb3\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"edc55e2c-231c-4186-8e75-b42496bfafb3\")) {                    Plotly.newPlot(                        \"edc55e2c-231c-4186-8e75-b42496bfafb3\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335],\"xaxis\":\"x\",\"y\":[1.6044490337371826,1.7026413679122925,1.7023118734359741,1.608899474143982,1.5937708616256714,1.658950924873352,1.650824785232544,1.643791913986206,1.7264541387557983,1.5992501974105835,1.7308849096298218,1.606921672821045,1.6930351257324219,1.6311030387878418,1.6365734338760376,1.598164439201355,1.6471261978149414,1.6574039459228516,1.5874384641647339,1.7060248851776123,1.6971620321273804,1.610618233680725,1.738182783126831,1.687888741493225,1.6635462045669556,1.6937917470932007,1.7057290077209473,1.7500381469726562,1.6394400596618652,1.7171146869659424,1.7342758178710938,1.714180827140808,1.755419373512268,1.5434229373931885,1.7344805002212524,1.6202611923217773,1.660550594329834,1.7091565132141113,1.6857209205627441,1.7428338527679443,1.7172234058380127,1.6416770219802856,1.654927134513855,1.648665189743042,1.5950530767440796,1.7025543451309204,1.708864688873291,1.6328322887420654,1.7189021110534668,1.6828557252883911,1.6582605838775635,1.6615055799484253,1.6817781925201416,1.6792877912521362,1.6703033447265625,1.6642563343048096,1.7802038192749023,1.745740532875061,1.64566969871521,1.6995652914047241,1.77035653591156,1.7146633863449097,1.7107458114624023,1.9174866676330566,1.8054499626159668,1.6350648403167725,1.6840598583221436,1.732898235321045,1.637035846710205,1.6366913318634033,1.6007016897201538,1.6472755670547485,1.6530728340148926,1.6781249046325684,1.6617162227630615,1.6485188007354736,1.6516882181167603,1.6435418128967285,1.690584659576416,1.7210252285003662,1.7846651077270508,1.699881911277771,1.6915053129196167,1.7590409517288208,1.6845670938491821,1.7585978507995605,1.4681955575942993,1.5826587677001953,1.6707279682159424,1.6677703857421875,1.576842188835144,1.5676579475402832,1.5976303815841675,1.6287519931793213,1.608169436454773,1.6396303176879883,1.5538721084594727,1.668794870376587,1.5515528917312622,1.6350799798965454,1.6095677614212036,1.607609510421753,1.555759072303772,1.6448776721954346,1.6022108793258667,1.5484005212783813,1.6224110126495361,1.6447365283966064,1.5693684816360474,1.67880117893219,1.6243665218353271,1.6279237270355225,1.6520028114318848,1.6154440641403198,1.6748762130737305,1.5957523584365845,1.6255332231521606,1.688106656074524,1.6308342218399048,1.7119652032852173,1.5329039096832275,1.7231109142303467,1.5786622762680054,1.6566365957260132,1.675258755683899,1.6278090476989746,1.7204190492630005,1.6696696281433105,1.603137731552124,1.6215728521347046,1.6401865482330322,1.595674991607666,1.6527609825134277,1.6667835712432861,1.6157587766647339,1.7022404670715332,1.6445437669754028,1.6388332843780518,1.6464258432388306,1.6730310916900635,1.650547742843628,1.663248896598816,1.6148765087127686,1.7142126560211182,1.6848154067993164,1.5827486515045166,1.6280263662338257,1.6697428226470947,1.645900845527649,1.6661423444747925,1.8033747673034668,1.773640751838684,1.602509617805481,1.6643214225769043,1.7210321426391602,1.6088391542434692,1.5993882417678833,1.5647478103637695,1.5734050273895264,1.5965678691864014,1.6203808784484863,1.639953374862671,1.583717703819275,1.6283081769943237,1.5808994770050049,1.641979694366455,1.6937404870986938,1.7477575540542603,1.6509981155395508,1.6161304712295532,1.681248664855957,1.6117384433746338,1.6771758794784546,1.4092519283294678,1.5289040803909302,1.5944254398345947,1.6678684949874878,1.5569686889648438,1.5486754179000854,1.5861104726791382,1.6124223470687866,1.5795572996139526,1.6070435047149658,1.5679426193237305,1.632538914680481,1.5143200159072876,1.580661654472351,1.5449035167694092,1.5434378385543823,1.5013225078582764,1.593076467514038,1.559990406036377,1.5076619386672974,1.5506058931350708,1.5770039558410645,1.498173475265503,1.6364281177520752,1.6110036373138428,1.6141293048858643,1.6457505226135254,1.6008529663085938,1.6506640911102295,1.5459939241409302,1.606378436088562,1.623038649559021,1.6021246910095215,1.6811915636062622,1.5127676725387573,1.7055470943450928,1.5474647283554077,1.6339080333709717,1.662122130393982,1.646151065826416,1.7090520858764648,1.6708990335464478,1.6048284769058228,1.607317328453064,1.6044549942016602,1.5829848051071167,1.6121995449066162,1.6293009519577026,1.638831377029419,1.665127158164978,1.6367952823638916,1.6568833589553833,1.6048269271850586,1.644428014755249,1.6050655841827393,1.6595286130905151,1.583268165588379,1.6967211961746216,1.6536054611206055,1.536051630973816,1.6008343696594238,1.6542710065841675,1.629260540008545,1.6317272186279297,1.768145203590393,1.7022480964660645,1.5551156997680664,1.5805552005767822,1.6457457542419434,1.596217155456543,1.5498625040054321,1.5212736129760742,1.5452104806900024,1.5969990491867065,1.597856879234314,1.6336948871612549,1.5514562129974365,1.5680350065231323,1.544958472251892,1.585250973701477,1.6226357221603394,1.6627106666564941,1.5477097034454346,1.5721393823623657,1.60330069065094,1.5866116285324097,1.6261705160140991,1.3722249269485474,1.4707080125808716,1.6030634641647339,1.62455153465271,1.4969459772109985,1.4908413887023926,1.542902946472168,1.5639657974243164,1.5320184230804443,1.5550240278244019,1.5325545072555542,1.595083236694336,1.480340838432312,1.5255367755889893,1.5245498418807983,1.5358459949493408,1.4800053834915161,1.5295822620391846,1.5219390392303467,1.4999144077301025,1.5206542015075684,1.5763236284255981,1.469478964805603,1.5556707382202148,1.5314924716949463,1.499460220336914,1.571751594543457,1.5660662651062012,1.6103094816207886,1.5331273078918457,1.600300908088684,1.5703147649765015,1.5459396839141846,1.6049891710281372,1.4612349271774292,1.6290086507797241,1.4914017915725708,1.5570393800735474,1.611493706703186,1.5855739116668701,1.6115608215332031,1.616181492805481,1.6085498332977295,1.5870153903961182,1.5784145593643188,1.544816493988037,1.540888786315918,1.5276542901992798,1.5493565797805786,1.6014928817749023,1.6010698080062866,1.588141918182373,1.5668203830718994,1.5784597396850586,1.5986590385437012,1.61164128780365,1.5818076133728027,1.653832197189331,1.6199591159820557,1.4973143339157104,1.5688294172286987,1.6161545515060425,1.5861073732376099,1.584887146949768,1.7236093282699585,1.7194868326187134,1.5664342641830444,1.5956662893295288,1.6039305925369263,1.5662283897399902,1.5555214881896973,1.5031951665878296,1.523478627204895,1.539078950881958,1.5184028148651123,1.5498425960540771],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('edc55e2c-231c-4186-8e75-b42496bfafb3');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "df=pd.DataFrame(perp)\n",
        "px.scatter(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOY_ESJSLqJP"
      },
      "source": [
        "EXO 2.3.A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxKKDv4ohHHW",
        "outputId": "b17860a9-8b56-4952-c37a-0cc36a274ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "A DONKEY once put on a Lion’s skin which some hunters had spread out\n",
            "to deserved to chooging all thing\n",
            "with her. “There is nothing that we want but that this fine weather and\n",
            "he drew for the\n",
            "Dovechowed on the world.”\n",
            "\n",
            "“What I have some yourued my life is nothing. At most it confes of the Hares said to his bell and crow.\n",
            "\n",
            "1. The moremest required to and bedous of Gromped out to dele his cries, as I well well,” said the Man, “to get them by\n",
            "ore to a Gardinging betaring of thes were get out, better death to \n"
          ]
        }
      ],
      "source": [
        "print(complete(model, \"THE DONKEY IN THE LION’S SKIN\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwDAqV8KLiof"
      },
      "source": [
        "EXO 2.3.B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ8kgfqt2pF9",
        "outputId": "43bf8407-4942-426a-ec32-c7090a608406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " AND THE GOWK\n",
            "\n",
            "\n",
            "A MOU AND THE LAMB\n",
            "\n",
            "\n",
            "A WOL Man once her adains, and flowing upon him all other work and you for do intells in\n",
            "labous neck was the mad in a housekeeping oven the braved, and you have resied, and edented har least horself ourself in its orm to his mother, and you little clance of the world. Let us have heary for the longer.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE COCK AND THE COLK\n",
            "\n",
            "\n",
            "A HUN who had all beciuned in the water, but one soon gets\n",
            "used himself in the dog sud lowaly for it is not\n",
            "amusing; and net an all don’t knowne\n"
          ]
        }
      ],
      "source": [
        "text=complete(model, \"THE DOG IN THE LONE HOUSE\", 512, sample=False)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JL17n-6Lb6-"
      },
      "source": [
        "EXO 2.3.C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E63WDHwsLNLK",
        "outputId": "3551f5d3-7118-4ad6-a793-0f45687afc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it. “When the charces as\n",
            "one of the lobted for a\n",
            "limbly and procied aroud aty, and electronic work, you into the beants.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE DOG IN THE LION’S SKIN\n",
            "\n",
            "\n",
            "A DONKEY once door leaped hardleds asong the coor.\n",
            "\n",
            "“Hor fie company, and at a holly gear by teres them and fand, and all the more.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE COCK AND THE COCK\n",
            "\n",
            "\n",
            "A HUN The Mouse so wherever in companions, lunging upon him all other work and you for do intells in\n",
            "labous neck was the mad in a housekeeping oven the braved, and you have resied, and edented har l\n"
          ]
        }
      ],
      "source": [
        "print(complete(model, \"The monkey eat \", 512, sample=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViG7Z5xJLxmy"
      },
      "source": [
        "EXO 2.3.D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNBwx4FDLNrd",
        "outputId": "4eda3c8d-0833-4623-ed0e-d0ec3f4425b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pooked upon her home. But as he could and cold for a provine.”\n",
            "\n",
            "The Fowler tree for him as hear of heary at\n",
            "even my good,” said the Fowler. “Wicked in a house.”\n",
            "\n",
            "The Fox so much of such a veighbor,” said her young one can only beautiful than the other lived and careful in leapand, and so well heary your veally and ede to cuar one mistaken for the edge of the works, who were deart, and at all the years and not in sight.”\n",
            "\n",
            "“When the laws of the country road\n",
            "proud of his trouble.\n",
            "\n",
            "“Lazy fellow,” said Bersuse\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(complete(model, \"Another bites to \", 512, sample=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb32YINUOlQi"
      },
      "source": [
        "#Model LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axmTiFskOkSs"
      },
      "outputs": [],
      "source": [
        "# LSTM based language model\n",
        "class LSTMModel(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, emb_dim, hidden_dim, num_layers):\n",
        "        \"\"\"Parameters:\n",
        "\n",
        "          num_classes (int): number of input/output classes\n",
        "          emb_dim (int): token embedding size\n",
        "          hidden_dim (int): hidden layer size of RNNs\n",
        "          num_layers (int): number of RNN layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.input_layer = nn.Embedding(num_classes, emb_dim,padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers)\n",
        "        self.out_layer = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, input,state):\n",
        "        emb = self.input_layer(input)\n",
        "        output, (hidden_state,cell_state) = self.lstm(emb, state)\n",
        "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
        "        output = self.out_layer(output)\n",
        "\n",
        "        return output, (hidden_state,cell_state)\n",
        "\n",
        "    def init_hidden(self, bsz):\n",
        "        weight = next(self.parameters())\n",
        "        return weight.new_zeros(self.num_layers, bsz, self.hidden_dim)\n",
        "\n",
        "# To be modified for LSTM...\n",
        "def custom_detach(h):\n",
        "    return h.detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xb-KLY-PH_D"
      },
      "source": [
        "#Decoding LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CPGgx0kDPHK9"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def complete2(model, prompt, steps, sample=False):\n",
        "    \"\"\"Complete the prompt for as long as given steps using the model.\n",
        "\n",
        "    Parameters:\n",
        "      model: language model\n",
        "      prompt (str): text segment to be completed\n",
        "      steps (int): number of decoding steps.\n",
        "      sample (bool): If True, sample from the model. Otherwise greedy.\n",
        "\n",
        "    Returns:\n",
        "      completed text (str)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    out_list = []\n",
        "\n",
        "    # forward the prompt, compute prompt's ppl\n",
        "    prompt_list = []\n",
        "    char_prompt = list(prompt)\n",
        "    for char in char_prompt:\n",
        "        prompt_list.append(my_data.vocab.string_to_id[char])\n",
        "    x = torch.tensor(prompt_list).to(DEVICE).unsqueeze(1)\n",
        "\n",
        "    hidden_state = model.init_hidden(1)\n",
        "    cell_state = model.init_hidden(1)\n",
        "    logits,(hidden_state,cell_state) = model(x, (hidden_state,cell_state))\n",
        "    probs = F.softmax(logits[-1], dim=-1)\n",
        "\n",
        "    if sample:\n",
        "        ix=torch.multinomial(probs, num_samples=1)\n",
        "        #assert False\n",
        "    else:\n",
        "        max_p, ix = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "    out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "    x = ix.unsqueeze(1)\n",
        "\n",
        "    # decode\n",
        "    for k in range(steps):\n",
        "        logits,(hidden_state,cell_state)= model(x, (hidden_state,cell_state))\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        if sample:  # sample from the distribution or take the most likely\n",
        "             ix=torch.multinomial(probs, num_samples=1)\n",
        "             #assert False\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        out_list.append(my_data.vocab.id_to_string[int(ix)])\n",
        "        x = ix\n",
        "    return ''.join(out_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12mJsR55POzf",
        "outputId": "65c7a3df-47d6-4048-f4ba-dae643b8ab14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size: 107\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "clipping = 1.0\n",
        "embedding_size = 64\n",
        "lstm_size = 2048\n",
        "lstm_num_layers = 1\n",
        "\n",
        "# vocab_size = len(module.vocab.itos)\n",
        "vocab_size = len(my_data.vocab.id_to_string)\n",
        "print(F\"vocab size: {vocab_size}\")\n",
        "\n",
        "model2 = LSTMModel(\n",
        "    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=lstm_size,\n",
        "    num_layers=lstm_num_layers)\n",
        "model2 = model2.to(DEVICE)\n",
        "hidden = model2.init_hidden(batch_size)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpdk9QoewMqX",
        "outputId": "869023d5-c3c1-48b2-c5d5-d4e375c88488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== start epoch 1 ===\n",
            "train ppl: 106.93266296386719\n",
            "----------------- epoch/batch 1/0 -----------------\n",
            "Dogs like best to\n",
            "                                                                                                                                 \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 13.471542358398438\n",
            "----------------- epoch/batch 1/30 -----------------\n",
            "Dogs like best to\n",
            " the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 10.902223587036133\n",
            "----------------- epoch/batch 1/60 -----------------\n",
            "Dogs like best to\n",
            " the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed the sed \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 2 ===\n",
            "train ppl: 8.557916641235352\n",
            "----------------- epoch/batch 2/0 -----------------\n",
            "Dogs like best to\n",
            " the wat the Found the wat the wat the wat the wat the wat the wat the wat the wat the wat the wat the wat the wat the wat the wa\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 7.0787577629089355\n",
            "----------------- epoch/batch 2/30 -----------------\n",
            "Dogs like best to\n",
            " the said the said the said the said the said the said the said the said the said the said the said the said the said the said th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 6.947437763214111\n",
            "----------------- epoch/batch 2/60 -----------------\n",
            "Dogs like best to\n",
            " the said the Work and the Work and the Work and the Work and the Work and the Work and the Work and the Work and the Work and th\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 3 ===\n",
            "train ppl: 6.091876029968262\n",
            "----------------- epoch/batch 3/0 -----------------\n",
            "Dogs like best to\n",
            " the water and the water and the water and the water and the water and the water and the water and the water and the water and th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.497936248779297\n",
            "----------------- epoch/batch 3/30 -----------------\n",
            "Dogs like best to\n",
            " be a strong to be and the strong to be and the strong to be and the strong to be and the strong to be and the strong to be and t\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 5.547028064727783\n",
            "----------------- epoch/batch 3/60 -----------------\n",
            "Dogs like best to\n",
            " the forth the forth with the forth the forth with the forth the forth with the forth the forth with the forth the forth with the\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 4 ===\n",
            "train ppl: 4.912047386169434\n",
            "----------------- epoch/batch 4/0 -----------------\n",
            "Dogs like best to\n",
            " the beat and the water and the water and the water and the water and the water and the water and the water and the water and the\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.668221473693848\n",
            "----------------- epoch/batch 4/30 -----------------\n",
            "Dogs like best to\n",
            " be a strong and the one of the one of the one of the one of the one of the one of the one of the one of the one of the one of th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.730304718017578\n",
            "----------------- epoch/batch 4/60 -----------------\n",
            "Dogs like best to\n",
            " the forest the forth the forth the forth the forth the forth the forth the forth the forth the forth the forth the forth the for\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 5 ===\n",
            "train ppl: 4.176521301269531\n",
            "----------------- epoch/batch 5/0 -----------------\n",
            "Dogs like best to\n",
            " the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trou\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.101789951324463\n",
            "----------------- epoch/batch 5/30 -----------------\n",
            "Dogs like best to\n",
            " be a stream and see that he had been the same one of his spreach of his friend, and the stream and he could not come and see tha\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 4.098117828369141\n",
            "----------------- epoch/batch 5/60 -----------------\n",
            "Dogs like best to\n",
            " the seed of the same to the seed of the same to the seed of the same to the seed of the same to the seed of the same to the seed\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 6 ===\n",
            "train ppl: 3.717825412750244\n",
            "----------------- epoch/batch 6/0 -----------------\n",
            "Dogs like best to\n",
            " the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trouble to the trou\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.5480456352233887\n",
            "----------------- epoch/batch 6/30 -----------------\n",
            "Dogs like best to\n",
            " the tent.”\n",
            "\n",
            "“You are you will be so that you will be so the tent of the stream. “If you will be so that you will be so the tent.\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.574695587158203\n",
            "----------------- epoch/batch 6/60 -----------------\n",
            "Dogs like best to\n",
            " see if the water, and the Wolf stopped to the forest.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE WOLF AND THE CRAT\n",
            "\n",
            "\n",
            "A WOLF saw a country where the Wolf and the Wo\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 7 ===\n",
            "train ppl: 3.3075950145721436\n",
            "----------------- epoch/batch 7/0 -----------------\n",
            "Dogs like best to\n",
            " the tree to the tree to the tree to the tree to the tree to the tree to the tree to the tree to the tree to the tree to the tree\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.2070038318634033\n",
            "----------------- epoch/batch 7/30 -----------------\n",
            "Dogs like best to\n",
            " be a big and see that the Wolf came to the strong began to the stream and the same companions, and the Wolf came to the stranger\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 3.1199915409088135\n",
            "----------------- epoch/batch 7/60 -----------------\n",
            "Dogs like best to\n",
            " the great stream.\n",
            "\n",
            "“What is true,” said the Wolf, “that it was not away.”\n",
            "\n",
            "“What is true,” said the Wolf, “that it was not away.\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 8 ===\n",
            "train ppl: 3.036369562149048\n",
            "----------------- epoch/batch 8/0 -----------------\n",
            "Dogs like best to\n",
            " make a stone creature you are a refund of the work is not a refund of the work is not a refund of the work is not a refund of th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.7851102352142334\n",
            "----------------- epoch/batch 8/30 -----------------\n",
            "Dogs like best to\n",
            " see how you are and get out of danger. Let us be a few forms of the world. The Wolf wished to see how you are and get out of som\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.759141683578491\n",
            "----------------- epoch/batch 8/60 -----------------\n",
            "Dogs like best to\n",
            " the ground and tried to\n",
            "the flock, and said to himself in the water, and said, “it is not all that you have a good pull in the w\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 9 ===\n",
            "train ppl: 2.6759016513824463\n",
            "----------------- epoch/batch 9/0 -----------------\n",
            "Dogs like best to\n",
            " the water who was not an and by he was not an and begged him to go for a strength in the world.\n",
            "\n",
            "“Oh, not in the water, and the \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.4604568481445312\n",
            "----------------- epoch/batch 9/30 -----------------\n",
            "Dogs like best to\n",
            " eat in his man.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FOX AND THE LION\n",
            "\n",
            "\n",
            "A WOLF, passing by, and the Fox were on who had lost his foolish for the day, who had\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.4680213928222656\n",
            "----------------- epoch/batch 9/60 -----------------\n",
            "Dogs like best to\n",
            " the stories fast in the water.\n",
            "\n",
            "“It is not make such as the water was so back to see what it was not away from you were down tha\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 10 ===\n",
            "train ppl: 2.3731255531311035\n",
            "----------------- epoch/batch 10/0 -----------------\n",
            "Dogs like best to\n",
            " the terms of the fable is not all the terms of the fable is not all the terms of the fable is not all the terms of the fable is \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.253354787826538\n",
            "----------------- epoch/batch 10/30 -----------------\n",
            "Dogs like best to\n",
            " the best of it. So the one who could not seen him come to the brook that he could set his boasting of the tree, and the Wolf dee\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 2.1772828102111816\n",
            "----------------- epoch/batch 10/60 -----------------\n",
            "Dogs like best to\n",
            " eat it, she saw that the Wolf saw him to the bottom\n",
            "of the forest into the beautiful. The\n",
            "Foxes are so lost all the best may be \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 11 ===\n",
            "train ppl: 2.1279637813568115\n",
            "----------------- epoch/batch 11/0 -----------------\n",
            "Dogs like best to\n",
            " the bell. “How did it happen that you must be strong and stood up and said, “I am sure, thank you.”\n",
            "\n",
            "“No, thank you because I ca\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9516631364822388\n",
            "----------------- epoch/batch 11/30 -----------------\n",
            "Dogs like best to\n",
            " take his life would like to rescue the some one to advise.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE WOLF AND THE LION\n",
            "\n",
            "\n",
            "A WOLF, passing one day, and the Wolf cam\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.9195507764816284\n",
            "----------------- epoch/batch 11/60 -----------------\n",
            "Dogs like best to\n",
            " eat it.\n",
            "\n",
            "On the creature were traveling together, and asked her nest atted the man in the water.\n",
            "\n",
            "The next time the Fox with his\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 12 ===\n",
            "train ppl: 1.9494911432266235\n",
            "----------------- epoch/batch 12/0 -----------------\n",
            "Dogs like best to\n",
            " enturned to his mother was true.\n",
            "But to ho served and brought up a blazing stick of her heart,\n",
            "for when they came to the greedy \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.7343474626541138\n",
            "----------------- epoch/batch 12/30 -----------------\n",
            "Dogs like best to\n",
            " the stag. “It is easy our foot to have a thind to see you to the destruction.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE WOLF AND THE LION\n",
            "\n",
            "\n",
            "A MONKEY and a Cock, w\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.736750602722168\n",
            "----------------- epoch/batch 12/60 -----------------\n",
            "Dogs like best to\n",
            " eat it along the hearts.\n",
            "\n",
            "Then said the man put a bridle in his mouth and see he to the best that he could not rise all the hard\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 13 ===\n",
            "train ppl: 1.74574875831604\n",
            "----------------- epoch/batch 13/0 -----------------\n",
            "Dogs like best to\n",
            " eat it at the way to\n",
            "the the substance.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE MOUNTAIN IN LABOR\n",
            "\n",
            "\n",
            "IN DAYS of old, a Fox was a beast so a farmer with a larger \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.56586754322052\n",
            "----------------- epoch/batch 13/30 -----------------\n",
            "Dogs like best to\n",
            " the water so that the\n",
            "most of his life in the danger. He was carrying them and called the Fox.\n",
            "\n",
            "“How can you have one more dance\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.5773009061813354\n",
            "----------------- epoch/batch 13/60 -----------------\n",
            "Dogs like best to\n",
            " eat it. As the water was so\n",
            "far from teeth to his time to the edgesooved to see if he could secure the same good\n",
            "fortune to his \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 14 ===\n",
            "train ppl: 1.6634910106658936\n",
            "----------------- epoch/batch 14/0 -----------------\n",
            "Dogs like best to\n",
            " eat and beast as well as to be which the preceding fables. The\n",
            "Eagle, to protect her nest and the other dog, who was eating his \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.4572885036468506\n",
            "----------------- epoch/batch 14/30 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "same time they said, the Fox could do with the stream, and the little Mice stole away to\n",
            "their work.\n",
            "\n",
            "One day the Mouse came\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.4826160669326782\n",
            "----------------- epoch/batch 14/60 -----------------\n",
            "Dogs like best to\n",
            " eat it.\n",
            "A Bricklayer gave his hunding round another dangered that it was not an and asked the honor to tell the hid\n",
            "with the lan\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 15 ===\n",
            "train ppl: 1.526024580001831\n",
            "----------------- epoch/batch 15/0 -----------------\n",
            "Dogs like best to\n",
            " help them, but he walked slowly off, grumbling as he did so:\n",
            "“The grapes are sour, and not at all fit for my back, I shall be in\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.3702200651168823\n",
            "----------------- epoch/batch 15/30 -----------------\n",
            "Dogs like best to\n",
            " eat their house.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FOX AND THE LITTLE FISH\n",
            "\n",
            "\n",
            "ALL day a Wolf and a Lamb happened to come at the same to adviser; but it is \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.3472410440444946\n",
            "----------------- epoch/batch 15/60 -----------------\n",
            "Dogs like best to\n",
            " eat it.\n",
            "A mother was the man in the hearing one to actroached the hole brought him to retire. There is troubled time to the pine\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 16 ===\n",
            "train ppl: 1.4576529264450073\n",
            "----------------- epoch/batch 16/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "to them to do with the refreshment of both man and beast.\n",
            "\n",
            "Seeing a large, strong them as possible, threw the\n",
            "mother way a m\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.3064744472503662\n",
            "----------------- epoch/batch 16/30 -----------------\n",
            "Dogs like best to\n",
            " eat the beasts that are\n",
            "not in sight. A morsel is better than the ones we are\n",
            "suffering from. There is lost nothing, and curring\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.2479450702667236\n",
            "----------------- epoch/batch 16/60 -----------------\n",
            "Dogs like best to\n",
            " eat it. What\n",
            "should have been conceived, and kicked in the\n",
            "water, and the Wolf saw a mark of the man, the Wolf stopped to\n",
            "safe i\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 17 ===\n",
            "train ppl: 1.3807165622711182\n",
            "----------------- epoch/batch 17/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "trademark license, especially to point some general truth or to draw a helpful lesson,\n",
            "no two versions of the same fable are\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.2286725044250488\n",
            "----------------- epoch/batch 17/30 -----------------\n",
            "Dogs like best to\n",
            " make up to the bush.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FOX AND THE LION\n",
            "\n",
            "\n",
            "A LITTLE fox was out playing one day, when boon and said, “Stop a moment,\n",
            "let us\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.2201359272003174\n",
            "----------------- epoch/batch 17/60 -----------------\n",
            "Dogs like best to\n",
            " eat it. As the\n",
            "water was pushed up by them to the stream.\n",
            "\n",
            "Then he saw that the other dog had lost his piece, too. He went sadly\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 18 ===\n",
            "train ppl: 1.3374357223510742\n",
            "----------------- epoch/batch 18/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "to the birds.\n",
            "\n",
            "“Have pity on me!” it begged. “I have been appointed Nightingale to\n",
            "these woods, and yet the birds dare laugh\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.194765329360962\n",
            "----------------- epoch/batch 18/30 -----------------\n",
            "Dogs like best to\n",
            " have\n",
            "means for him. And the Stork, who could only dip the tip of\n",
            "his bill in the dish all the shaderard. “Oh, oh, I know you\n",
            "wil\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.1729072332382202\n",
            "----------------- epoch/batch 18/60 -----------------\n",
            "Dogs like best to\n",
            " lame, up in\n",
            "her beak, a Fox chanced to pass by saw him there, a prive, heard him go, not so much from\n",
            "either sight. A morsel is \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 19 ===\n",
            "train ppl: 1.301573395729065\n",
            "----------------- epoch/batch 19/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "trademark license, especially commercial redistribute this\n",
            "electronic work, or any part of this Project Gutenberg-tm web sit\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.1449834108352661\n",
            "----------------- epoch/batch 19/30 -----------------\n",
            "Dogs like best to\n",
            " have\n",
            "good sure to bring themselves of the animals he wished to make his prey,\n",
            "and, waiting his opportunity, seize them and bear \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.139180064201355\n",
            "----------------- epoch/batch 19/60 -----------------\n",
            "Dogs like best to\n",
            " eat it.”\n",
            "\n",
            "Upon seeing this was her beak, a Fox chanced to pass by, and\n",
            "looking up saw her. “How good that cheese smells!” though\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 20 ===\n",
            "train ppl: 1.2855111360549927\n",
            "----------------- epoch/batch 20/0 -----------------\n",
            "Dogs like best to\n",
            " eat at a mouse, the Father should\n",
            "be such a friend who do so constantly.\n",
            "\n",
            "The Fox, looking up, said: “Dear Mr. Cock, you see how\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.1064940690994263\n",
            "----------------- epoch/batch 20/30 -----------------\n",
            "Dogs like best to\n",
            " eat it. What there is trouble there is need of some one to act.\n",
            "\n",
            "“Thank you, my little friend,” she said, “is as much ours as me\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.124940276145935\n",
            "----------------- epoch/batch 20/60 -----------------\n",
            "Dogs like best to\n",
            " eat it.\n",
            "\n",
            "“Why do you torture the terth.”\n",
            "\n",
            "“But did you see the whelp of a good plan. If you will put your\n",
            "forefeet high up on th\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 21 ===\n",
            "train ppl: 1.2379834651947021\n",
            "----------------- epoch/batch 21/0 -----------------\n",
            "Dogs like best to\n",
            " walk straight into it and was burned.\n",
            "\n",
            "“What!” said the Father, “if you wish for fine weather, and your sister\n",
            "for rain, which s\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.1064749956130981\n",
            "----------------- epoch/batch 21/30 -----------------\n",
            "Dogs like best to\n",
            " the bottom of the lake, a wise old Hare said, “Stop a moment!\n",
            "let us consider. Here are creatures that are more timid that he sh\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.091036081314087\n",
            "----------------- epoch/batch 21/60 -----------------\n",
            "Dogs like best to\n",
            " make them dive, and I would have gone near her, but that drew forth to give you a sheep and drought it out safe?”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE ANTS A\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 22 ===\n",
            "train ppl: 1.2178096771240234\n",
            "----------------- epoch/batch 22/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "trademark license, especially commercial redistribution.\n",
            "\n",
            "START: FULL LICENSE\n",
            "\n",
            "THE FULL PROJECT GUTENBERG LICENSE\n",
            "PLEASE REA\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.091956377029419\n",
            "----------------- epoch/batch 22/30 -----------------\n",
            "Dogs like best to\n",
            " the bottom of the lake, a wise old Hare said, “Stop a moment!\n",
            "let us consider. Here are creatures that are more timid than we—th\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.094513177871704\n",
            "----------------- epoch/batch 22/60 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "status of any work in any\n",
            "country outside the United States.\n",
            "\n",
            "1.E. Unless you have removed all references to Project Gutenbe\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 23 ===\n",
            "train ppl: 1.2214062213897705\n",
            "----------------- epoch/batch 23/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "ground and soon convinced him that he was not an Elephant.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FULL AND THE GOAT\n",
            "\n",
            "\n",
            "A WOLF saw a Goat feeding at the top \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0636192560195923\n",
            "----------------- epoch/batch 23/30 -----------------\n",
            "Dogs like best to\n",
            " exton them.\n",
            "\n",
            "“What a soon came? I am a Bird. Look at my wings. This\n",
            "is my battle as truly as it is yours.”\n",
            "\n",
            "But the Porcupine wa\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.067943811416626\n",
            "----------------- epoch/batch 23/60 -----------------\n",
            "Dogs like best to\n",
            " the tree, and threw\n",
            "him a leaf. “Climb up on that leaf,” said she, “and you will float\n",
            "ashore.”\n",
            "\n",
            "The Ant climbed up onto the lea\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 24 ===\n",
            "train ppl: 1.1764075756072998\n",
            "----------------- epoch/batch 24/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "trademark owner, and a puff of wind blew it out.\n",
            "\n",
            "As the owner relighted it, he said: “Cease now your beard, you would\n",
            "have \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0598596334457397\n",
            "----------------- epoch/batch 24/30 -----------------\n",
            "Dogs like best to\n",
            " eat the Lamb, but meeting her as he did,\n",
            "face to face, he thought he must find some excuse for doing so.\n",
            "\n",
            "So he began by trying \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0643370151519775\n",
            "----------------- epoch/batch 24/60 -----------------\n",
            "Dogs like best to\n",
            " eat the Lamb, but meeting her as he did,\n",
            "face to face, she troubled the Fox, snapping up, ate as he walked away,\n",
            "remarking that \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 25 ===\n",
            "train ppl: 1.1738044023513794\n",
            "----------------- epoch/batch 25/0 -----------------\n",
            "Dogs like best to\n",
            " the\n",
            "grass all day, up and down the hills and\n",
            "along the brook; and all that the Boy had to do was to look upon him.\n",
            "\n",
            "“We shall be\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0583887100219727\n",
            "----------------- epoch/batch 25/30 -----------------\n",
            "Dogs like best to\n",
            " eat it. The\n",
            "same tried in every way to\n",
            "get out, but at last began to think that it was impossible, and that he\n",
            "must die there, a\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0588752031326294\n",
            "----------------- epoch/batch 25/60 -----------------\n",
            "Dogs like best to\n",
            " eat at home,\n",
            "and waved her long tail from side to side uneasily, and presently\n",
            "said: “It is awkward standing thus. It would take\n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 26 ===\n",
            "train ppl: 1.1523678302764893\n",
            "----------------- epoch/batch 26/0 -----------------\n",
            "Dogs like best to\n",
            " the bell who had showly die; for the\n",
            "completion of their rulers. One of these missions, undertaken at\n",
            "the command of Crœsus, was\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0597975254058838\n",
            "----------------- epoch/batch 26/30 -----------------\n",
            "Dogs like best to\n",
            " eat the Lamb, but meeting her as he did,\n",
            "face to face, he thought he must find some excuse for doing so.\n",
            "\n",
            "So he began by trying \n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.0561491250991821\n",
            "----------------- epoch/batch 26/60 -----------------\n",
            "Dogs like best to\n",
            " take him one by one, and so raised the water to\n",
            "the brim and quenched his thirst.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE ANT AND THE DOVE\n",
            "\n",
            "\n",
            "AN Ant, walking by \n",
            "----------------- end generated text -------------------\n",
            "=== start epoch 27 ===\n",
            "train ppl: 1.1462066173553467\n",
            "----------------- epoch/batch 27/0 -----------------\n",
            "Dogs like best to\n",
            " eat it.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE BOASTING TRAVELER\n",
            "\n",
            "\n",
            "A MAN who had traveled in foreign parts bragged, on his return home,\n",
            "of the great feats he h\n",
            "----------------- end generated text -------------------\n",
            "train ppl: 1.042366623878479\n",
            "----------------- epoch/batch 27/30 -----------------\n",
            "Dogs like best to\n",
            " eat the Lamb, but meeting her as he did,\n",
            "face to face, he thought he must find some excuse for doing so.\n",
            "\n",
            "So he began by trying \n",
            "----------------- end generated text -------------------\n",
            "final perplexiity :  1.0287106 \n",
            "\n",
            "final Epoch : 27\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "\n",
        "num_epoch =0\n",
        "report_every = 30\n",
        "prompt = \"Dogs like best to\"\n",
        "perp=50\n",
        "\n",
        "while perp > 1.03:\n",
        "    num_epoch+=1\n",
        "\n",
        "\n",
        "    print(f\"=== start epoch {num_epoch} ===\")\n",
        "\n",
        "    hidden_state = model2.init_hidden(batch_size)\n",
        "    cell_state = model2.init_hidden(batch_size)\n",
        "    for idx in range(len(batches)):\n",
        "        batch = batches[idx]\n",
        "        model2.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input = batch[:-1]\n",
        "        target = batch[1:].flatten()\n",
        "\n",
        "        output,(hidden_state,cell_state)= model2(input, (hidden_state,cell_state))\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "        perp=torch.exp(loss)\n",
        "        perp=perp.to(\"cpu\").detach().numpy()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "\n",
        "        hidden_state=custom_detach(hidden_state)\n",
        "        cell_state=custom_detach(cell_state)\n",
        "        if perp < 1.03:\n",
        "          break\n",
        "        if idx % report_every == 0:\n",
        "            # print(f\"train loss: {loss.item()}\")  # replace me by the line below!\n",
        "            print(f\"train ppl: {perp}\")\n",
        "            generated_text = complete2(model2, prompt, 128, sample=False)\n",
        "            print(f'----------------- epoch/batch {num_epoch}/{idx} -----------------')\n",
        "            print(prompt)\n",
        "            print(generated_text)\n",
        "            print(f'----------------- end generated text -------------------')\n",
        "\n",
        "print(\"final perplexiity : \",perp,\"\\n\")\n",
        "print(\"final Epoch :\",num_epoch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "co1F89Cxzb2g"
      },
      "source": [
        "#EXO 3.4.A\n",
        "\n",
        " Title of a fable which exists in the book."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1eCKWVh1osV",
        "outputId": "bf6c3e52-9e47-4fe4-f0f0-513984801db9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " feed upon the Lamb, the Wolf left few\n",
            "of them uninjured.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE FISHERMAN AND THE LITTLE FISH\n",
            "\n",
            "\n",
            "ALL day long a Fisherman had been toiling and had caught nothing.\n",
            "\n",
            "“I will make one more effort,” thought he, “and then I must go home.”\n",
            "\n",
            "He threw in his line, and soon drew up a very small perch.\n",
            "\n",
            "The little Fish was terribly frightened when he found himself out of\n",
            "water, and with a sharp hook sticking in his mouth; and he said to the\n",
            "Fisherman:\n",
            "\n",
            "“O sir, take pity upon me, and throw me into the water again! See\n"
          ]
        }
      ],
      "source": [
        "print(complete2(model2, \"The Fox and the Goat\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYDOUJ69NyNo",
        "outputId": "3cd67d9b-507f-49db-a893-d48898435c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " for hunger, ruphing that home, whe house he was thinking how sad that\n",
            "when he grew no what we are clow\n",
            "indering that he slow to go what it was impossible, and that he\n",
            "must die there, a prisoner. While he was thinking how sad that would\n",
            "be, a thirsty Goat came and looked down intortanity, but mine from a comparison will open the water so\n",
            "that I cannot drive its best to eat the Lamb, he said, “You little wretch,\n",
            "if it was not you it was your father, so it’s all the simple creidure that I have spoiled the wate\n"
          ]
        }
      ],
      "source": [
        "print(complete2(model2, \"The Fox and the Goat\", 512, sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cY6TDPazyoH"
      },
      "source": [
        "# EXO 3.4.B\n",
        "title which invented,is not in the book, but similar in the style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "nFE_iJdduPPH",
        "outputId": "2e4e1403-0d24-4ec5-cf5a-2d19c6101fbd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'n for her home. But it was not long before they found that he slew more\\nof them in a single day than the Kite could possibly pounce upon in a\\nwhole year.\\n\\nThe oldest, wisest pigeon among them said: “When we are in trouble,\\nwe must not forget that there are other dangers than the ones we are\\nsuffering from. There is a proverb among men that tells them to avoid a\\nremedy that is worse than the disease.”\\n\\n\\n\\n\\nTHE WAR HORSE AND THE MULE\\n\\n\\nA WAR Horse, ready for battle, with his splendid saddle and jingling\\nbridle,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "print(complete2(model2, \"The Serpent and the Cow\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "iOekHa2GN1aK",
        "outputId": "7b7959f5-9b8b-4d6b-ec4b-fe01a32c4005"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'n came running home to her child,\\ntherefore, that while, for his promise’s sake, he would give her the\\npower to harm, she must be careful to him and showed them a bundle of the food, and the Hawk to let her go.\\n\\n“If you are hungry,” said she, “why not catch some large bird? I am not\\nbig enough for even a luncheon.”\\n\\n“Do you happen to see many large birds flying about?” the Hawk asked. “I should be\\nfoolish, indeed, to let you go for the sake of larger birds that are hilding, then I must go home.” So he\\nhad en'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "print(complete2(model2, \"The Serpent and the Cow\", 512, sample=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jdEYT0G7xzO"
      },
      "source": [
        "#Bonus\n",
        "Train model with python script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVQNVaqw5Qtv"
      },
      "outputs": [],
      "source": [
        "# This is for Colab. Adapt the path if needed.\n",
        "text_path = \"/content/python.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "e-D-4ApG5Qtv",
        "outputId": "f93b025d-8187-4f1c-99f1-7952320cda7d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-26013b7b49ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbptt_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataBatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-50fbcc36b4f0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, vocab, extend_vocab, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextend_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-50fbcc36b4f0>\u001b[0m in \u001b[0;36mtext_to_data\u001b[0;34m(self, text_file, vocab, extend_vocab, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \"\"\"\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = TextData(text_path, device=DEVICE)\n",
        "batches = DataBatches(my_data, batch_size, bptt_len, pad_id=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZDz7OtN5sYk"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "clipping = 1.0\n",
        "embedding_size = 64\n",
        "lstm_size = 2048\n",
        "lstm_num_layers = 1\n",
        "\n",
        "# vocab_size = len(module.vocab.itos)\n",
        "vocab_size = len(my_data.vocab.id_to_string)\n",
        "print(F\"vocab size: {vocab_size}\")\n",
        "\n",
        "model2 = LSTMModel(\n",
        "    num_classes=vocab_size, emb_dim=embedding_size, hidden_dim=lstm_size,\n",
        "    num_layers=lstm_num_layers)\n",
        "model2 = model2.to(DEVICE)\n",
        "hidden = model2.init_hidden(batch_size)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(params=model2.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jgb7qFGw50_J"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "num_epoch =0\n",
        "report_every = 30\n",
        "prompt = \"Def func \"\n",
        "perp=3\n",
        "while perp > 1.03:\n",
        "    num_epoch+=1\n",
        "\n",
        "\n",
        "    print(f\"=== start epoch {num_epoch} ===\")\n",
        "\n",
        "    hidden_state = model2.init_hidden(batch_size)\n",
        "    cell_state = model2.init_hidden(batch_size)\n",
        "    for idx in range(len(batches)):\n",
        "        batch = batches[idx]\n",
        "        model2.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input = batch[:-1]\n",
        "        target = batch[1:].flatten()\n",
        "\n",
        "        output,(hidden_state,cell_state)= model2(input, (hidden_state,cell_state))\n",
        "\n",
        "        loss = loss_fn(output, target)\n",
        "        perp=torch.exp(loss)\n",
        "        perp=perp.to(\"cpu\").detach().numpy()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), clipping)\n",
        "        optimizer.step()\n",
        "\n",
        "        hidden_state=custom_detach(hidden_state)\n",
        "        cell_state=custom_detach(cell_state)\n",
        "        if perp < 1.03:\n",
        "          break\n",
        "        if idx % report_every == 0:\n",
        "            # print(f\"train loss: {loss.item()}\")  # replace me by the line below!\n",
        "            print(f\"train ppl: {perp}\")\n",
        "            generated_text = complete2(model2, prompt, 128, sample=False)\n",
        "            print(f'----------------- epoch/batch {num_epoch}/{idx} -----------------')\n",
        "            print(prompt)\n",
        "            print(generated_text)\n",
        "            print(f'----------------- end generated text -------------------')\n",
        "\n",
        "print(\"final perplexiity : \",perp,\"\\n\")\n",
        "print(\"final Epoch :\",num_epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWJCrVKK7ubj"
      },
      "outputs": [],
      "source": [
        "print(complete2(model2, \"print(\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0JHq_My7v3J"
      },
      "outputs": [],
      "source": [
        "print(complete2(model2, \"If \", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_-NJ6jl7xNx"
      },
      "outputs": [],
      "source": [
        "print(complete2(model2, \"while\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E03-enX27xzD"
      },
      "outputs": [],
      "source": [
        "print(complete2(model2, \"for i in range\", 512, sample=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgm8tAiX8Euy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}